---
title: "Intro to Decision Trees"
output: 
  learnr::tutorial:
    css: "css/style.css"
    progressive: true
    allow_skip: true
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
# Author: Al Morgan
# Original Date: Dec 2024
# Version of R: 3.6.1

library(learnr)
library(gradethis)
library(readr)
library(dplyr)
library(ggplot2)
library(rpart)
library(rpart.plot)
knitr::opts_chunk$set(echo = FALSE)

tutorial_options(
  exercise.checker = gradethis::grade_learnr
)

titanic_data <- read_csv("www/data/titanic_decision_tree_data.csv")
set.seed(19)
```

```{r phs-logo, echo=FALSE, fig.align='right', out.width="40%"}
knitr::include_graphics("images/phs-logo.png")
```


## Introduction

Welcome to an Introduction to Decision Trees. This course is designed as a self-led introduction to decision trees for anyone in Public Health Scotland. Throughout this course there will be quizzes to test your knowledge and opportunities to modify and write R code. Below is an overview of the learning pathway.

```{r intro-pathway, echo=FALSE, fig.align='center', out.width="100%"}
knitr::include_graphics("images/r-intro-pathway-2.png")
```

<div class="info_box">
  <h4>Course Info</h4>
  <ul>
    <li>This course is built to flow through sections and build on previous knowledge. If you're comfortable with a particular section, you can skip it.</li>
    <li>Most sections have multiple parts to them. Navigate the course by using the buttons at the bottom of the screen to Continue or go to the Next Topic.</li>
    <li>The course will also show progress through sections, a green tick will appear on sections you've completed, and it will remember your place if you decide to close your browser and come back later.</li>
  </ul>
</div>
</br>

### What is a decision tree?

* A useful type of model that allow us to make predictions from our data
* A map of the possible outcomes of a series of related choices, often seen in businesses as ‘decision flowcharts’ or ‘process flowcharts’
* A series of choices/questions that are read downwards, that lead to a final prediction for any given data point. 
Similar to a regression model (see Intro to Linear Regression), we can build a decision tree using our data, and subsequently predict the chosen outcome variable based on the predictor variables used to build a model.
The useful part of a decision tree is that the predicted outcome variable can be continuous (e.g. if I consumed four eggs, two slices of toast, and a cup of coffee for breakfast, how many miles will I cycle today?) or categorical (e.g. if it’s a Monday morning with high wind and rain, will John go for a bike ride, yes or no?). turn into a diagram?


<div>
```{r echo=FALSE, fig.align='center', out.width="75%"}
knitr::include_graphics("images/illustrations/r_welcome_twitter.png")
```
</div>

</br>

Since we're getting started, here's a quiz to get familiar with the layout:

```{r intro-quiz}
quiz(
  question("Which of the following can R help to produce?",
    answer("Tidy Data", correct = TRUE),
    answer("Dashboards", correct = TRUE),
    answer("Static/Interactive Reports", correct = TRUE),
    answer("Web Apps", correct = TRUE),
    answer("Magic", message = "It may seem like R is magic but unfortunately not"),
    answer("Databases", correct = TRUE),
    answer("Presentations", correct = TRUE),
    incorrect = "Not quite, have another go!",
    allow_retry = TRUE,
    random_answer_order = TRUE
  )
)
```
## Goal of this course

In this course, we’re going to build a decision tree based off a dataset about passengers onboard the Titanic. This model will allow us to predict who was most likely to survive the unfortunate sinking of the vessel (apologies for the morbid nature of this data!).

To do this we’re going to:
1.	Read in the relevant data
2.	Wrangle the data and perform some **feature engineering**
3.	Create some simple plots to help identify useful predictors for our model
4.	Build a decision tree model using `rpart()` from the `{rpart}` package
5.	Visualise the decision tree using `rpart.plot()` from the `{rpart.plot}` package
6.	Interpret the model
7.	Test the model’s predictive power by comparing predicted values to those in our data
Following this, we will create a **random forest** model and see how if this improves the predictive power we can attain.

<br>

## Exploring the dataset

We can read in our Titanic data using `read_csv()` from the `{dplyr}` package:
```{r, echo = TRUE, eval = TRUE}
library(dplyr)
titanic_data <- read_csv("www/data/titanic_decision_tree_data.csv")

# check out the first 10 rows of the data
head(titanic_data)
```

### Data dictionary

What variables do we have here?
•	X1: the default column name given by R to this column which is simply the row number
•	passenger_id: a unique number given to each passenger
•	pclass : ticket class, 1 = 1st (upper class), 2 = 2nd (middle class), 3 = 3rd (lower class)
•	name: passenger name
•	sex: biological sex, male or female
•	age: passenger age
•	sib_sp : number of siblings / spouses on board
•	parch: number of parents / children aboard the Titanic; some children travelled only with a nanny, therefore parch=0 for them
•	ticket: ticket ID
•	fare: the cost of the passenger’s ticket
•	cabin: cabin number (if known/relevant)
•	embarked: city of embarkation; C = Cherbourg, Q = Queenstown, S = Southampton
•	survived : did they survive? 0 = No, 1 = Yes

<br>

## Data wrangling and feature engineering

Prior to building a model, cleaning and wrangling our data is always the first step.

Here is what we’ll do in this section:
1.	Filter down to only observations which have a survived flag (i.e. that aren’t missing)
2.	Turn the relevant variables into factors (sex, survived, pclass, embarkation)
o	This way the model will identify sex e.g. as a factor with 2 levels as opposed to a character variable
3.	Create an age_status variable which groups individuals under (and including) 16 years of age into a category called “child” category and those over 16 into a category called “adult”.
o	This is an example of **feature engineering** in which we are creating a child/adult variable based on a variable we already have, age
4.	Drop any variables we don’t need
o	It’s important to identify and remove the variables which will be of no use inside a decision tree prior to building it
o	We’ll remove those variables that are unique to each row and can’t be grouped into meaningful categories: X1, passenger_id, name, ticket
o	We’ll remove fare and cabin as the information in these is better represented in the categorical pclass variable already 
5.	Drop any rows with NA
o	It’s important to do this after we’ve trimmed down our data to the columns we’re interested in as otherwise we sacrifice rows for the sake of variables we don’t want anyway

Try having a go at the above list of cleaning/wrangling tasks on your own before looking at the answers below.
```{r wrangle-input, exercise = TRUE}
titanic_clean <- titanic_data %>%
## start coding here
```

<details>
<summary>Data cleaning code</summary>

```{r, echo = TRUE, eval = TRUE}
titanic_clean <- titanic_data %>%
# Filter out those without a survived flag
  filter(survived %in% c(0,1)) %>%
# Convert to factor level
    mutate(sex = as.factor(sex), 
           age_status = as.factor(if_else(age <= 16, "child", "adult")),
         class = factor(pclass, levels = c(3, 2, 1), labels = c("Lower", "Middle", "Upper")), 
           survived_flag = factor(survived, levels = c(0, 1), labels = c("No", "Yes")), 
           port_embarkation = as.factor(embarked)) %>%
# Select variables of interest
  select(sex, age_status, class, port_embarkation, sib_sp, parch, survived_flag) %>%
  tidyr::drop_na()

head(titanic_clean)
```
</details>

Now our data is trimmed and consists of a) the outcome variable of interest, the `survived` flag, and b) a collection of potential predictors that may be useful in our model when predicting `survived`.

<br>

## Visualise the data

This isn’t a strictly necessary step when building a decision tree, but it’s good practice to understand your data before building a model with it. Creating some quick visualisations is a great way to better understand your data.

Use ggplot2 to create some visualisations to see which variables appear to have the greatest effect on survival.
```{r ggplot-input, exercise = TRUE}
titanic_clean <- titanic_data %>%
## start coding here
```
Once we create the model in the next section, we’ll see if your predictions were correct.
<br>

## Build and plot the decision tree

In this section, we’re going to build the decision tree itself. However, we will want to **test** the performance of the model on some of our `titanic_clean` data. For this reason, we will split the data randomly into an 80%/20% split: building the model on 80% of the data and holding aside 20% to test if the predicted `survival` values it returns are similar to the true values in the data.

### Train/test split

The following code will split the data into ‘training’ and ‘testing’ subsets.
```{r, echo = TRUE, eval = TRUE}
# number of rows in the data
n_data <- nrow(titanic_clean)

# create a test sample index, i.e. a collection of row numbers for the test sample
test_index <- sample(1:n_data, size = n_data*0.2)

# create the test set, i.e. keep rows from the data using the index above
titanic_test  <- slice(titanic_clean, test_index)

# create the training set, i.e. remove rows from the data using the index above
titanic_train <- slice(titanic_clean, -test_index)
```

This now gives us two data frames to work with: the larger training data `titanic_train` with which we’ll build the decision tree, and the smaller training data `titanic_train` with which we’ll test the tree with later. It’s important that we leave this training data aside until then.

We can check how *balanced* the data frames are, in terms of our outcome variable of interest `survived`, by using the `tabyl()` function from the `{janitor}` package. If one side of the train/test split had the large majority of passengers who survived, and the other side had the large majority of passengers who did not, it would be a good idea to resample the data.

```{r, echo = TRUE, eval = TRUE}
# check for balanced sets
titanic_train %>%
 	janitor::tabyl(survived_flag)
```
This shows us that x% of passengers in the training data survived and y% did not.

```{r, echo = TRUE, eval = TRUE}
titanic_test %>%
 janitor::tabyl(survived_flag)
```
And this shows us that y% of passengers in the testing data survived and x% did not.

It’s very unlikely that we’ll get an exact match between data sets, but in general the gap between the two will be smaller with larger datasets. For now, this seems like a pretty even split.

###

## Interpreting decision trees
## Testing the decision tree

## Random forest





```{r foundations-basics-quiz}
quiz(
  question("What would `typeof(as.logical(0))` return and what is its value?",
           answer("Numeric - `0`"),
           answer('Character - `"0"`'),
           answer("Logical - `TRUE`"),
           answer("Logical - `FALSE`", correct = TRUE),
           incorrect = "Not quite, have another go!",
           allow_retry = TRUE,
           random_answer_order = TRUE
           ),
  question("If `x = 5`, what does this return: `x < 10 || x == 4`?",
           answer("TRUE", correct = TRUE),
           answer("FALSE"),
           incorrect = "Not quite, have another go!",
           allow_retry = TRUE,
           random_answer_order = TRUE
           ),
  question("What makes an **invalid** name for a variable?",
           answer("Starting with an underscore (`_`)", correct = TRUE),
           answer("Starting with a dot (`.`)"),
           answer("Symbols other than an underscore (`_`) or dot (`.`)", correct = TRUE),
           answer("Starting with a number", correct = TRUE),
           answer("Reserved names, e.g. `TRUE`", correct = TRUE),
           incorrect = "Not quite, have another go!",
           allow_retry = TRUE,
           random_answer_order = TRUE
           )
)
```



## Help & Feedback

Now it's time to embed your new knowledge and skills, expand with related technologies (e.g. git), and when you're ready *Take R Further* with more training opportunities.

#### Debugging

<div>
```{r echo=FALSE, fig.align='center', out.width="75%"}
knitr::include_graphics("images/illustrations/r_debugging.jpg")
```
</div>

1. **Review warnings/errors** - these can appear cryptic but use Google and some will become familiar. Checking the functions could help too - `?<function>`
2. **Narrow the problem** - step through the code, isolating the issue to a specific chunk, line, or function.
3. **Google/StackOverflow** - this can be specific to the bug or more general to the
problem you’re trying to solve. You should tag queries with "[r]".
4. **Pair up** - sometimes a fresh pair of eyes makes the difference (you might have just missed a comma). For a more general audience post a message on the R User Group [Technical Queries Teams channel](https://teams.microsoft.com/l/channel/19:9620ef6cf8234d50a0f95caba65a3edf@thread.tacv2/Technical%2520Queries?groupId=ec4250f9-b70a-4f32-9372-a232ccb4f713&tenantId=10efe0bd-a030-4bca-809c-b5e6745e499a).

</br>

#### Help

<div>
```{r echo=FALSE, fig.align='center', out.width="75%"}
knitr::include_graphics("images/illustrations/r_rollercoaster.png")
```
</div>

* [R User Group Teams](https://teams.microsoft.com/l/team/19%3ae9f55a12b7d94ef49877ff455a07f035%40thread.tacv2/conversations?groupId=ec4250f9-b70a-4f32-9372-a232ccb4f713&tenantId=10efe0bd-a030-4bca-809c-b5e6745e499a) / [Technical Queries](https://teams.microsoft.com/l/channel/19%3a9620ef6cf8234d50a0f95caba65a3edf%40thread.tacv2/Technical%2520Queries?groupId=ec4250f9-b70a-4f32-9372-a232ccb4f713&tenantId=10efe0bd-a030-4bca-809c-b5e6745e499a)
* [Transforming Publishing Team email](mailto:phs.transformingpublishing@nhs.net?subject=Introduction to R Training Online - Help)

#### Feedback

<iframe width="100%" height= "2300" src= "https://forms.office.com/Pages/ResponsePage.aspx?id=veDvEDCgykuAnLXmdF5JmibxHi_yzZ9Pvduh8IqoF_5UQzVRTUE2NEZPQktaR1BUMkZKWE05S1lETSQlQCN0PWcu&embed=true" frameborder= "0" marginwidth= "0" marginheight= "0" style= "border: none; max-width:100%; max-height:100vh" allowfullscreen webkitallowfullscreen mozallowfullscreen msallowfullscreen> </iframe>
